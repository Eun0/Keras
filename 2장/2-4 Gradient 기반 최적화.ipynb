{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련(Training)이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 계산은 다음과 같다\n",
    "\n",
    "output=relu(dot(W,input)+b)\n",
    "\n",
    "여기서 W와 b를 가중치(weight) or 훈련되는 파라미터(trainable parameter)\n",
    "\n",
    "각각 W를 커널, b를 편향(bias)이라고 한다\n",
    "\n",
    "훈련(Training)이란 원하는 결과가 나오도록 훈련되는 파라미터 값을 최적화 시키는 것을 말한다\n",
    "\n",
    "(즉, loss가 최소가 되도록 w,b를 업데이트!)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 반복 루프(Training loop)\n",
    "\n",
    "훈련 반복 루프를 통해 훈련이 일어난다\n",
    "\n",
    "1. input에 상응하는 label의 배치를 추출\n",
    "\n",
    "2. input으로 네트워크 실행, label을 예측\n",
    "\n",
    "3. 예측 label과 실제 label의 차이를 측정하여 손실을 계산\n",
    "\n",
    "4. 배치에 대한 손실이 감소되도록 네트워크의 가중치를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How 가중치 update?\n",
    "\n",
    "신경망에 사용되는 모든 연산 __미분가능__하므로,\n",
    "\n",
    "__'손실에 대한 gradient의 반대 방향으로 가중치 이동'__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확률적 경사 하강법 (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss를 최대한 작게 만드는 것이 학습의 목표\n",
    "\n",
    "미분 가능한 함수는 최솟값 해석적으로 구할 수 있다\n",
    "\n",
    "How?\n",
    "\n",
    "=> 미분해서 0 되는 지점\n",
    "\n",
    "\n",
    "### 미니 배치 확륙적 경사 하강법\n",
    "\n",
    ": gradient의 반대 qjdguddnfg 가중치 update해서 미분해서 0되는 지점으로 이동\n",
    "\n",
    "가중치 update 알고리즘\n",
    "\n",
    "1. 훈련 샘플 배치 input와 이에 상응하는 label을 추출\n",
    "\n",
    "2. input으로 네트워크를 실행하고 label을 예측\n",
    "\n",
    "3. 예측 label과 실제 label의 차이를 측정하여 네트워크의 손실을 계산\n",
    "\n",
    "4. 네트워크의 파라미터에 대한 손실함수의 gradient를 계산 ( __역전파(Backpropagation)__ 이용)\n",
    "\n",
    "5. gradient의 반대 방향으로 파라미터를 조금 이동 ( __W-=step*gradient__ )=> 배치에 대한 손실 조금 감소\n",
    "\n",
    "여기서 얼마나 내려갈 지 (감소) 정하는 __step__값을 적절히 고르는 것이 중요\n",
    "\n",
    "- step이 너무 크면 => 발산\n",
    "\n",
    "- step이 너무 작으면 => 학습 속도 너무 느리고, local minimum으로 수렴한다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 update하는 또 다른 방법들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치를 update하는 방법을 __'optimizer'__라고 부른다\n",
    "\n",
    "위에서 설명한 방법이 SGD이다\n",
    "\n",
    "다음과 같은 optimizer들이 있다\n",
    "\n",
    "- SGD\n",
    "\n",
    "- Adagrad\n",
    "\n",
    "- RMSProp\n",
    "\n",
    "- Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
